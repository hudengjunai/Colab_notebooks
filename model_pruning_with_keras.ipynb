{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model pruning with keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hudengjunai/Colab_notebooks/blob/master/model_pruning_with_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNVmv70dLeqV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "53694233-c169-4953-aef4-bf8ec2843701"
      },
      "source": [
        "!pip list|grep tensorflow\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mesh-tensorflow               0.0.5                \n",
            "tensorflow-estimator          1.14.0               \n",
            "tensorflow-hub                0.5.0                \n",
            "tensorflow-metadata           0.14.0               \n",
            "tensorflow-model-optimization 0.1.2                \n",
            "tensorflow-probability        0.7.0                \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0816 02:08:15.801848 140323773405056 __init__.py:689] \n",
            "\n",
            "  TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0.\n",
            "\n",
            "  Please upgrade your code to TensorFlow 2.0:\n",
            "    * https://www.tensorflow.org/beta/guide/migration_guide\n",
            "\n",
            "  Or install the latest stable TensorFlow 1.X release:\n",
            "    * `pip install -U \"tensorflow==1.*\"`\n",
            "\n",
            "  Otherwise your code may be broken by the change.\n",
            "\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6H6zX8YLxTk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "90d104e9-ab6d-42ae-d7d5-3aaaac810959"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0-dev20190815\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04LF3lruJxgY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 943
        },
        "outputId": "5bc2f293-84aa-4053-e57b-72e65ee1e268"
      },
      "source": [
        "!pip uninstall -y tensorflow\n",
        "!pip uninstall -y tf-nightly\n",
        "!pip install -U tf-nightly-gpu\n",
        "!pip install tensorflow-model-optimization\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.14.0:\n",
            "  Successfully uninstalled tensorflow-1.14.0\n",
            "\u001b[33mWARNING: Skipping tf-nightly as it is not installed.\u001b[0m\n",
            "Collecting tf-nightly-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/70/a103b288fbb14af8dc84a5f02394a6cd851c5b011f75dc25fc1720a4cd70/tf_nightly_gpu-1.15.0.dev20190815-cp36-cp36m-manylinux2010_x86_64.whl (410.5MB)\n",
            "\u001b[K     |████████████████████████████████| 410.5MB 50kB/s \n",
            "\u001b[?25hCollecting tf-estimator-nightly (from tf-nightly-gpu)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/6b/5287e1b19092b67f0151c6a0ad320f0257f2ec5f42dd93cfdfb2310f53c4/tf_estimator_nightly-1.14.0.dev2019081501-py2.py3-none-any.whl (502kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 51.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.16.4)\n",
            "Collecting opt-einsum>=2.3.2 (from tf-nightly-gpu)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/7b/8eeb7a6e0dcb23b49dfb38c33d6a640c7cb2375973ca2f0597725f024c01/opt_einsum-3.0.0.tar.gz (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 32.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.7.1)\n",
            "Collecting tb-nightly<1.16.0a0,>=1.15.0a0 (from tf-nightly-gpu)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/2a/8a12aa8b5a656f40144c5f7d23635d43c9fa17d08ba0c8a01f341252b05b/tb_nightly-1.15.0a20190815-py3-none-any.whl (4.1MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1MB 51.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (3.7.1)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.1.7)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.33.4)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tf-nightly-gpu) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.16.0a0,>=1.15.0a0->tf-nightly-gpu) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.16.0a0,>=1.15.0a0->tf-nightly-gpu) (41.0.1)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.16.0a0,>=1.15.0a0->tf-nightly-gpu) (0.15.5)\n",
            "Building wheels for collected packages: opt-einsum\n",
            "  Building wheel for opt-einsum (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for opt-einsum: filename=opt_einsum-3.0.0-cp36-none-any.whl size=58490 sha256=8572179a52ae11a9fb1f927d3aead7d4d9e3cdb7b5a3de7a67102ba4fcacb6b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/82/aa/e6d68e36a66706a69f890a6072793d2b87730572499c626ed8\n",
            "Successfully built opt-einsum\n",
            "Installing collected packages: tf-estimator-nightly, opt-einsum, tb-nightly, tf-nightly-gpu\n",
            "Successfully installed opt-einsum-3.0.0 tb-nightly-1.15.0a20190815 tf-estimator-nightly-1.14.0.dev2019081501 tf-nightly-gpu-1.15.0.dev20190815\n",
            "Collecting tensorflow-model-optimization\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/1f/fe3a08323112c1b8ac248a56f96b395a6d0f96d42ce26e9e794a2543fa5e/tensorflow_model_optimization-0.1.2-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 26.4MB/s \n",
            "\u001b[?25hCollecting enum34~=1.1 (from tensorflow-model-optimization)\n",
            "  Downloading https://files.pythonhosted.org/packages/af/42/cb9355df32c69b553e72a2e28daee25d1611d2c0d9c272aa1d34204205b2/enum34-1.1.6-py3-none-any.whl\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.16.4)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.12.0)\n",
            "Installing collected packages: enum34, tensorflow-model-optimization\n",
            "Successfully installed enum34-1.1.6 tensorflow-model-optimization-0.1.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "enum"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc_oiK19Kx0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gODPY9rjKicf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "import tensorboard\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "import tempfile\n",
        "import zipfile\n",
        "import os\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqqVS-0hL3bd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6e9b81e-475c-4b16-ac33-38dfc3ebb4dc"
      },
      "source": [
        "tf.keras.backend.image_data_format()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'channels_last'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqtvffZEK0LJ",
        "colab_type": "text"
      },
      "source": [
        "## Prepare the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTuOE9KbKyeE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 10\n",
        "\n",
        "img_rows, img_cols = 28,28\n",
        "\n",
        "(x_train,y_train),(x_test,y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "if tf.keras.backend.image_data_format() == 'channels_first':\n",
        "  x_train = x_train.reshape(x_train.shape[0],1,img_rows,img_clols)\n",
        "  x_test = x_test.reshape(x_test.shape[0],1,img_rows,img_cols)\n",
        "  input_shape = (1,img_rows,img_cols)\n",
        "else:\n",
        "  x_train = x_train.reshape(x_train.shape[0],img_rows,img_cols,1)\n",
        "  x_test = x_test.reshape(x_test.shape[0],img_rows,img_cols,1)\n",
        "  input_shape = (img_rows,img_cols,1)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McslC807MKh_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "22894a40-2c2a-417f-92d6-fa61218c78fa"
      },
      "source": [
        "import numpy as np\n",
        "x_train = x_train.astype(np.float32)\n",
        "x_test = x_test.astype(np.float32)\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print(\"x train shape\",x_train.shape)\n",
        "print(\"x test shape \",x_test.shape)\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train,num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test,num_classes)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x train shape (60000, 28, 28, 1)\n",
            "x test shape  (10000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHKD8uYKMpmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tf.keras.utils.to_categorical(y_train,num_classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgYiyRYNNBfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E823H9nVNHvi",
        "colab_type": "text"
      },
      "source": [
        "Train a Mnist base model without pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDm96WUzNMqB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "e70b097f-424d-4cea-affa-c5a6e2b29909"
      },
      "source": [
        "layer = tf.keras.layers\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    layer.Conv2D(32,5,padding='same',activation='relu',input_shape=input_shape),\n",
        "    layer.MaxPooling2D((2,2),(2,2),padding='same'),\n",
        "    layer.BatchNormalization(),\n",
        "    layer.Conv2D(64,5,padding='same',activation='relu'),\n",
        "    layer.MaxPooling2D((2,2),(2,2),padding='same'),\n",
        "    layer.Flatten(),\n",
        "    layer.Dense(1024,activation='relu'),\n",
        "    layer.Dropout(0.4),\n",
        "    layer.Dense(num_classes,activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 32)        832       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 14, 14, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 64)        51264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              3212288   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 3,274,762\n",
            "Trainable params: 3,274,698\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym5b-JfkNxmV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e1656c4-532c-42d9-be91-c6b2cde14754"
      },
      "source": [
        "logdir = tempfile.mkdtemp()\n",
        "\n",
        "print(\"Writing training logs to \",logdir)\n",
        "%tensorboard --logdir={logdir}"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing training logs to  /tmp/tmp38p90x0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div id=\"root\"></div>\n",
              "    <script>\n",
              "      (function() {\n",
              "        window.TENSORBOARD_ENV = window.TENSORBOARD_ENV || {};\n",
              "        window.TENSORBOARD_ENV[\"IN_COLAB\"] = true;\n",
              "        document.querySelector(\"base\").href = \"https://localhost:6006\";\n",
              "        function fixUpTensorboard(root) {\n",
              "          const tftb = root.querySelector(\"tf-tensorboard\");\n",
              "          // Disable the fragment manipulation behavior in Colab. Not\n",
              "          // only is the behavior not useful (as the iframe's location\n",
              "          // is not visible to the user), it causes TensorBoard's usage\n",
              "          // of `window.replace` to navigate away from the page and to\n",
              "          // the `localhost:<port>` URL specified by the base URI, which\n",
              "          // in turn causes the frame to (likely) crash.\n",
              "          tftb.removeAttribute(\"use-hash\");\n",
              "        }\n",
              "        function executeAllScripts(root) {\n",
              "          // When `script` elements are inserted into the DOM by\n",
              "          // assigning to an element's `innerHTML`, the scripts are not\n",
              "          // executed. Thus, we manually re-insert these scripts so that\n",
              "          // TensorBoard can initialize itself.\n",
              "          for (const script of root.querySelectorAll(\"script\")) {\n",
              "            const newScript = document.createElement(\"script\");\n",
              "            newScript.type = script.type;\n",
              "            newScript.textContent = script.textContent;\n",
              "            root.appendChild(newScript);\n",
              "            script.remove();\n",
              "          }\n",
              "        }\n",
              "        function setHeight(root, height) {\n",
              "          // We set the height dynamically after the TensorBoard UI has\n",
              "          // been initialized. This avoids an intermediate state in\n",
              "          // which the container plus the UI become taller than the\n",
              "          // final width and cause the Colab output frame to be\n",
              "          // permanently resized, eventually leading to an empty\n",
              "          // vertical gap below the TensorBoard UI. It's not clear\n",
              "          // exactly what causes this problematic intermediate state,\n",
              "          // but setting the height late seems to fix it.\n",
              "          root.style.height = `${height}px`;\n",
              "        }\n",
              "        const root = document.getElementById(\"root\");\n",
              "        fetch(\".\")\n",
              "          .then((x) => x.text())\n",
              "          .then((html) => void (root.innerHTML = html))\n",
              "          .then(() => fixUpTensorboard(root))\n",
              "          .then(() => executeAllScripts(root))\n",
              "          .then(() => setHeight(root, 800));\n",
              "      })();\n",
              "    </script>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv7NMEUqWK_m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "b515a931-7ea4-4128-c335-b70960454972"
      },
      "source": [
        "callbacks = [tf.keras.callbacks.TensorBoard(log_dir=logdir,profile_batch=0)]\n",
        "\n",
        "model.compile(loss = tf.keras.losses.categorical_crossentropy,\n",
        "              optimizer = 'adam',\n",
        "              metrics= ['accuracy']\n",
        "             )\n",
        "\n",
        "model.fit(x_train,y_train,\n",
        "          batch_size = batch_size,\n",
        "          epochs = epochs,\n",
        "          verbose =1,\n",
        "          callbacks=callbacks,\n",
        "          validation_data=(x_test,y_test))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0816 02:58:12.403311 140323773405056 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:466: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Apply a constraint manually following the optimizer update step.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 10s 168us/sample - loss: 0.1763 - acc: 0.9512 - val_loss: 0.1121 - val_acc: 0.9840\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 4s 70us/sample - loss: 0.0454 - acc: 0.9856 - val_loss: 0.0308 - val_acc: 0.9898\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 4s 72us/sample - loss: 0.0313 - acc: 0.9902 - val_loss: 0.0354 - val_acc: 0.9877\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 4s 73us/sample - loss: 0.0236 - acc: 0.9925 - val_loss: 0.0239 - val_acc: 0.9925\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 4s 74us/sample - loss: 0.0194 - acc: 0.9938 - val_loss: 0.0309 - val_acc: 0.9909\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 4s 73us/sample - loss: 0.0181 - acc: 0.9941 - val_loss: 0.0224 - val_acc: 0.9935\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 4s 73us/sample - loss: 0.0142 - acc: 0.9955 - val_loss: 0.0281 - val_acc: 0.9914\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 4s 72us/sample - loss: 0.0130 - acc: 0.9960 - val_loss: 0.0315 - val_acc: 0.9913\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 4s 72us/sample - loss: 0.0128 - acc: 0.9956 - val_loss: 0.0336 - val_acc: 0.9917\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 4s 72us/sample - loss: 0.0129 - acc: 0.9962 - val_loss: 0.0329 - val_acc: 0.9915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9f206b4780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3lq63qpWq7B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a89f8946-f04c-453f-c9d6-8752ab02b2f8"
      },
      "source": [
        "score = model.evaluate(x_test,y_test,verbose=0)\n",
        "print(\"test loss\",score[0])\n",
        "print(\"test accuracy\",score[1])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test loss 0.03291126734791658\n",
            "test accuracy 0.9915\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss_t2SEhXThG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "643a69fc-59f4-4483-fc43-4366ba691f63"
      },
      "source": [
        "#batckend agnostic way to save/restore models\n",
        "\n",
        "_,keras_file = tempfile.mkstemp('.h5')\n",
        "\n",
        "print(\"saving model to \",keras_file)\n",
        "\n",
        "tf.keras.models.save_model(model,keras_file,include_optimizer=False)\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "saving model to  /tmp/tmp75mx44pr.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTaWcXkkXqSH",
        "colab_type": "text"
      },
      "source": [
        "## Train a pruned MNIST model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYZsgkVkXpW6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "285d416a-2a85-4026-9034-76721758a74d"
      },
      "source": [
        "import tensorflow_model_optimization\n",
        "dir(tensorflow_model_optimization.sparsity.keras)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ConstantSparsity',\n",
              " 'InputLayer',\n",
              " 'K',\n",
              " 'PolynomialDecay',\n",
              " 'PrunableLayer',\n",
              " 'PruningSchedule',\n",
              " 'PruningSummaries',\n",
              " 'UpdatePruningStep',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " 'abc',\n",
              " 'absolute_import',\n",
              " 'callbacks',\n",
              " 'constant_op',\n",
              " 'custom_object_scope',\n",
              " 'division',\n",
              " 'dtypes',\n",
              " 'keras',\n",
              " 'math_ops',\n",
              " 'np',\n",
              " 'ops',\n",
              " 'print_function',\n",
              " 'prune_low_magnitude',\n",
              " 'prune_scope',\n",
              " 'pruning_sched',\n",
              " 'pruning_wrapper',\n",
              " 'six',\n",
              " 'strip_pruning',\n",
              " 'tf']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQh5o8nMXoPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow_model_optimization.sparsity.keras import PolynomialDecay,prune_low_magnitude"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1RuNbPSZI30",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b6ab6241-f2d0-444a-a6e5-dee6174d5765"
      },
      "source": [
        "import numpy as np\n",
        "epochs = 12\n",
        "num_train_samples = x_train.shape[0]\n",
        "\n",
        "end_step = np.ceil(1.0*num_train_samples /batch_size).astype(np.int32)*epochs\n",
        "\n",
        "print(\"End Step \" +str(end_step))\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "End Step 5628\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzKNzRQ8ZcAE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "aa486bbd-a30c-4c2e-d986-aeac8ee7d840"
      },
      "source": [
        "pruning_params = {'pruning_schedule':PolynomialDecay(initial_sparsity=0.5,\\\n",
        "                                                    final_sparsity=0.9,\\\n",
        "                                                    begin_step = 2000,\n",
        "                                                    end_step=end_step,\\\n",
        "                                                    frequency=100)\n",
        "                 }\n",
        "\n",
        "pruned_model = tf.keras.Sequential([\n",
        "    prune_low_magnitude(layer.Conv2D(32,5,padding='same',activation='relu',\\\n",
        "                       input_shape=input_shape),**pruning_params),\n",
        "    layer.MaxPooling2D((2,2),(2,2),padding='same'),\n",
        "    layer.BatchNormalization(),\n",
        "    prune_low_magnitude(layer.Conv2D(64,5,padding='same',activation='relu'),\\\n",
        "                       **pruning_params),\n",
        "    layer.MaxPooling2D((2,2),(2,2),padding='same'),\n",
        "    layer.Flatten(),\n",
        "    prune_low_magnitude(layer.Dense(1024,activation='relu'),**pruning_params),\n",
        "    layer.Dropout(0.4),\n",
        "    prune_low_magnitude(layer.Dense(num_classes,activation='softmax'),**pruning_params)\n",
        "    \n",
        "])\n",
        "\n",
        "pruned_model.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0816 03:22:49.766052 140323773405056 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:185: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "W0816 03:22:50.018997 140323773405056 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_schedule.py:240: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "prune_low_magnitude_conv2d_2 (None, 28, 28, 32)        1634      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 14, 14, 32)        128       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_3 (None, 14, 14, 64)        102466    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_2  (None, 1024)              6423554   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_3  (None, 10)                20492     \n",
            "=================================================================\n",
            "Total params: 6,548,274\n",
            "Trainable params: 3,274,698\n",
            "Non-trainable params: 3,273,576\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k75pmnQaayV_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "109b6980-e88f-429d-aeac-a40d1ed8be42"
      },
      "source": [
        "logdir = tempfile.mkdtemp()\n",
        "print(\"writing train logs to \"+logdir)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "writing train logs to /tmp/tmpht_po467\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXwiM8oHa0vG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "02a376ee-f8b5-4231-b902-052fe97f2d70"
      },
      "source": [
        "pruned_model.compile(loss=tf.keras.losses.categorical_crossentropy,\\\n",
        "                     optimizer = 'adam',\\\n",
        "                     metrics = ['accuracy'])\n",
        "\n",
        "callbacks = [UpdatePruningStep(),PruningSummaries(log_dir=logdir,profile_batch=0)]\n",
        "\n",
        "pruned_model.fit(x_train,y_train,\n",
        "                batch_size=batch_size,\n",
        "                epochs=10,\n",
        "                verbose=1,\n",
        "                callbacks=callbacks,\n",
        "                validation_data=(x_test,y_test))\n",
        "\n",
        "score = pruned_model.evaluate(x_test,y_test,verbose=0)\n",
        "\n",
        "print(\"test score\",score[0])\n",
        "\n",
        "print(\"Test accuracy\",score[1])\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1825 - acc: 0.9496 - val_loss: 0.1017 - val_acc: 0.9811\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 6s 94us/sample - loss: 0.0487 - acc: 0.9850 - val_loss: 0.0292 - val_acc: 0.9910\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 6s 93us/sample - loss: 0.0317 - acc: 0.9901 - val_loss: 0.0274 - val_acc: 0.9906\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 6s 93us/sample - loss: 0.0246 - acc: 0.9919 - val_loss: 0.0282 - val_acc: 0.9900\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 6s 95us/sample - loss: 0.0198 - acc: 0.9940 - val_loss: 0.0228 - val_acc: 0.9920\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 6s 96us/sample - loss: 0.0161 - acc: 0.9947 - val_loss: 0.0257 - val_acc: 0.9923\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 6s 96us/sample - loss: 0.0153 - acc: 0.9951 - val_loss: 0.0237 - val_acc: 0.9926\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 6s 96us/sample - loss: 0.0144 - acc: 0.9955 - val_loss: 0.0215 - val_acc: 0.9932\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 6s 95us/sample - loss: 0.0145 - acc: 0.9951 - val_loss: 0.0255 - val_acc: 0.9918\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 6s 95us/sample - loss: 0.0122 - acc: 0.9959 - val_loss: 0.0261 - val_acc: 0.9908\n",
            "test score 0.026099640852197445\n",
            "Test accuracy 0.9908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wgrEGGCltWT",
        "colab_type": "text"
      },
      "source": [
        "## Save and restor the pruned model\n",
        "Continue to train for two epochs "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXc3moDBlqqO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "18a3756f-4d1e-4dd4-e1f5-2544f2699fd1"
      },
      "source": [
        "\n",
        "_,checkpoint_file = tempfile.mkstemp('.h5')\n",
        "print(\"saving pruned model to\",checkpoint_file)\n",
        "\n",
        "tf.keras.models.save_model(pruned_model,checkpoint_file,include_optimizer=True)\n",
        "\n",
        "with prune_scope():\n",
        "  restore_model = tf.keras.models.load_model(checkpoint_file)\n",
        "  \n",
        "restore_model.fit(x_train,y_train,\n",
        "                 batch_size=batch_size,\n",
        "                 epochs=2,\n",
        "                 verbose=1,\n",
        "                 callbacks=callbacks,\n",
        "                 validation_data=(x_test,y_test))\n",
        "\n",
        "score = restore_model.evaluate(x_test,y_test,verbose=0)\n",
        "\n",
        "print('test loss',score[0])\n",
        "print(\"test accuracy\",score[1])\n",
        "\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "saving pruned model to /tmp/tmpuxiepae1.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0816 06:08:58.646966 140323773405056 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1428: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 0.0112 - acc: 0.9964 - val_loss: 0.0256 - val_acc: 0.9921\n",
            "Epoch 2/2\n",
            "60000/60000 [==============================] - 6s 96us/sample - loss: 0.0069 - acc: 0.9977 - val_loss: 0.0253 - val_acc: 0.9927\n",
            "test loss 0.02529340742647055\n",
            "test accuracy 0.9927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjLIc370C1Si",
        "colab_type": "text"
      },
      "source": [
        "# strip the pruning model and then export for serving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdFsjCfHlpnS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "622b17c5-c9ea-4a3e-b338-9174183ba32a"
      },
      "source": [
        "final_model = strip_pruning(pruned_model)\n",
        "final_model.summary()\n",
        "\n",
        "_,pruned_keras_file = tempfile.mkstemp('.h5')\n",
        "print(\"saving pruned model to \",pruned_keras_file)\n",
        "\n",
        "tf.keras.models.save_model(final_model,pruned_keras_file,include_optimizer=False)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 28, 28, 32)        832       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 14, 14, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 14, 14, 64)        51264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1024)              3212288   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 3,274,762\n",
            "Trainable params: 3,274,698\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n",
            "saving pruned model to  /tmp/tmp285hu8a_.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stflJwtlJ124",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "bc3a3fc9-c376-41ae-813a-de189bb334e0"
      },
      "source": [
        "!ls /tmp"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__pycache__\ttmp75mx44pr.h5\t tmpj75fs6oa.py   tmpzshmr1cv.py\n",
            "tmp285hu8a_.h5\ttmp8ah2ubv9.py\t tmp_pgx0xll.zip\n",
            "tmp38p90x0s\ttmpcziusg4d.zip  tmptij_cf15.py\n",
            "tmp4q15klbo.h5\ttmpht_po467\t tmpuxiepae1.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPvF9bHDDeYS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "25dff587-17b3-4391-e6c7-66bd42e675e9"
      },
      "source": [
        "_,zip1 = tempfile.mkstemp('.zip')\n",
        "with zipfile.ZipFile(zip1,'w',compression=zipfile.ZIP_DEFLATED) as f:\n",
        "  f.write(keras_file)\n",
        "print(\"size of the unpruned model before compression :%.2f Mb\"%\n",
        "     (os.path.getsize(keras_file)/float(2**10)))\n",
        "\n",
        "print(\"size of the unpruned model after compression :%.2f Mb\"%\n",
        "     (os.path.getsize(zip1)/float(2**10)))\n",
        "\n",
        "_,zip2 = tempfile.mkstemp('.zip')\n",
        "with zipfile.ZipFile(zip2,'w',compression=zipfile.ZIP_DEFLATED) as f:\n",
        "  f.write(pruned_keras_file)\n",
        "  \n",
        "  print(\"size of the pruned model before compression :%.2f Mb\"%\n",
        "       (os.path.getsize(pruned_keras_file)/float(2**10)))\n",
        "  print(\"size of the pruned model fater compression :%.2f Mb\"%\n",
        "       (os.path.getsize(zip2)/float(2**10)))\n",
        "  "
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size of the unpruned model before compression :12817.91 Mb\n",
            "size of the unpruned model after compression :11865.78 Mb\n",
            "size of the pruned model before compression :12818.05 Mb\n",
            "size of the pruned model fater compression :2569.83 Mb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-RPYD4oKKYJ",
        "colab_type": "text"
      },
      "source": [
        "## so the pruned model after compression ,the size if 5x of the original compression model size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9vjm6WWJI0N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "58719cb6-5afb-4675-d4c7-8d86afe07546"
      },
      "source": [
        "tflite_model_file = '/tmp/sparse_mnist.tflite'\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model_file(pruned_keras_file)\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(tflite_model_file,'wb') as f:\n",
        "  f.write(tflite_model)\n",
        "  \n",
        "_,zip_tflite = tempfile.mkstemp('.zi')\n",
        "\n",
        "with zipfile.ZipFile(zip_tflite,'w',compression=zipfile.ZIP_DEFLATED) as f:\n",
        "  f.write(tflite_model_file)\n",
        "print(\"size of the tflite model before compression : %.2f Mb\"\n",
        "     %(os.path.getsize(tflite_model_file)/float(2**10)))\n",
        "print(\"size of the tflite model after compression :%.2f Mb\"\n",
        "     %(os.path.getsize(zip_tflite)/float(2**10)))\n",
        "\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0816 07:45:11.219341 140323773405056 hdf5_format.py:171] No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "size of the tflite model before compression : 12794.81 Mb\n",
            "size of the tflite model after compression :2488.21 Mb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQb26hcGY9BL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "70da11e4-2442-4617-edff-d3825d7d178f"
      },
      "source": [
        "import numpy as np\n",
        "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "\n",
        "output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "def eval_model(interperter,x_test,y_test):\n",
        "  total_seen = 0\n",
        "  num_correct = 0\n",
        "  \n",
        "  for img,label in zip(x_test,y_test):\n",
        "    inp = img.reshape((1,28,28,1))\n",
        "    total_seen +=1\n",
        "    \n",
        "    interpreter.set_tensor(input_index,inp)\n",
        "    interpreter.invoke()\n",
        "    \n",
        "    predictions = interpreter.get_tensor(output_index)\n",
        "    \n",
        "    if np.argmax(predictions) == np.argmax(label):\n",
        "      num_correct +=1 \n",
        "    \n",
        "    if total_seen %1000==0:\n",
        "      print(\"Accuracy after %i images :%f\"%\n",
        "           (total_seen,float(num_correct)/float(total_seen)))\n",
        "      \n",
        "  return float(num_correct)/float(total_seen)\n",
        "print(eval_model(interpreter,x_test,y_test))\n",
        "\n",
        "\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy after 1000 images :0.992000\n",
            "Accuracy after 2000 images :0.986500\n",
            "Accuracy after 3000 images :0.985667\n",
            "Accuracy after 4000 images :0.986500\n",
            "Accuracy after 5000 images :0.986000\n",
            "Accuracy after 6000 images :0.988000\n",
            "Accuracy after 7000 images :0.988857\n",
            "Accuracy after 8000 images :0.990000\n",
            "Accuracy after 9000 images :0.990556\n",
            "Accuracy after 10000 images :0.990800\n",
            "0.9908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyLgYWPdaXTj",
        "colab_type": "text"
      },
      "source": [
        "Post-trianing quantize the tensorflow lite model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaPeldW0aWj6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "df0aa3e4-0e32-4ee1-ee78-db291704ff04"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model_file(pruned_keras_file)\n",
        "\n",
        "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "tflite_quant_model_file = '/tmp/sparse_mnist_quant.tflite'\n",
        "\n",
        "with open(tflite_quant_model_file,'wb') as f:\n",
        "  f.write(tflite_quant_model)\n",
        "  "
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0816 08:00:50.206250 140323773405056 hdf5_format.py:171] No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHx-sDlQcdCe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "21751051-0a8d-4dd9-b835-eca497ec28eb"
      },
      "source": [
        "_,zip_tflite_quant = tempfile.mkstemp('.zip')\n",
        "with zipfile.ZipFile(zip_tflite_quant,'w',compression=zipfile.ZIP_DEFLATED) as f:\n",
        "  f.write(tflite_quant_model_file)\n",
        "  \n",
        "  print(\"Size of the tflite model before compression is %.2f\"%\n",
        "       (os.path.getsize(tflite_quant_model_file)/float(2**10)))\n",
        "  print(\"Size of the tflite model after compression is %.2f\"%\n",
        "       (os.path.getsize(zip_tflite_quant)/float(2**10)))\n",
        "  "
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of the tflite model before compression is 3206.89\n",
            "Size of the tflite model after compression is 608.07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwZkZbeFccy7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f50ec962-5769-4647-966d-9e431fd4a87f"
      },
      "source": [
        "interpreter = tf.lite.Interpreter(model_path = str(tflite_quant_model_file))\n",
        "interpreter.allocate_tensors()\n",
        "input_index = interpreter.get_input_details()[0]['index']\n",
        "output_index = interpreter.get_output_details()[0]['index']\n",
        "\n",
        "print(eval_model(interpreter,x_test,y_test))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy after 1000 images :0.992000\n",
            "Accuracy after 2000 images :0.986000\n",
            "Accuracy after 3000 images :0.985333\n",
            "Accuracy after 4000 images :0.986500\n",
            "Accuracy after 5000 images :0.985800\n",
            "Accuracy after 6000 images :0.987833\n",
            "Accuracy after 7000 images :0.988714\n",
            "Accuracy after 8000 images :0.989875\n",
            "Accuracy after 9000 images :0.990444\n",
            "Accuracy after 10000 images :0.990700\n",
            "0.9907\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEKUTBsIKINE",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}